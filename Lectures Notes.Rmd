---
title: "R Notebook"
output: 
  html_notebook: 
    toc: yes
---

# Week 1
## Matrix Derivatives

If we want to explain a vector $y_{n \times 1}$ with $X_{n \times p}$, we need a vector $\beta_{p \times 1}$ that minimize $$\lVert y_{n \times 1} - X_{n \times p}\beta_{p \times 1} \rVert$$
And the minimum of this expression is $$ \hat{\beta} = (x^tx)^{-1}x^ty $$

### Example
```{r}
data(mtcars)
head(mtcars)
```
We define our y, x and $\beta$ variables

\%\*\% matrix multiplication

solve(a) calculates inverse of a

solve(a,b) solves equation a %*% x = b

```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
beta = solve(t(x) %*% x) %*% t(x) %*% y
beta
```
We can check that we obtain the same vector than if we use the function lm
```{r}
coef(lm(mpg ~ wt + hp, data = mtcars))
```
## Centering by matrix multiplication
Let $J_n$ be a vector full of 1 of length n. Then, the matrix $$ (I - J_n(J_n^tJ_n)^{-1}J_n^t) = (I - \frac{1}{n}J_{n \times n}) $$ with $J_{n \times n}$ a matrix full of 1, multiplying $$ (I - \frac{1}{n}J_{n \times n})X_{n \times p} $$ will mean-center the columns of X

### Example

```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
n = nrow(x)
I = diag(rep(1,n))
H = matrix(1,n,n)/n
xc = (I-H) %*% x
```

If we calculate the mean by columns then we get zeros

```{r}
apply(xc,2,mean)
```
Same thing the other way around

```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
p = 3
I = diag(rep(1,p))
H = matrix(1,p,p)/p
xc = x %*%(I-H) 
apply(xc,1,mean)
```
 More efficient way using function sweep
 
```{r}
xc2 = sweep(x,2,apply(x,2,mean))
apply(xc2,2,mean)
```
## Variance via matrix multiplication
 
$$ S^2 = \frac{\sum(y_i-\bar{y})^2}{n-1} = \frac{\lVert y - \bar{y}J_n \rVert^2}{n-1} = \frac{1}{n-1}\tilde{y}^t\tilde{y} $$ where $\tilde{y}=(I - J_n(J_n^tJ_n)^{-1}J_n^t)y$

Then $$ S^2 = y^t(I - J_n(J_n^tJ_n)y $$

If we do it with $X_{n \times p}$ then $$ X^t(I - J_n(J_n^tJ_n)X = \tilde{X}^t\tilde{X} $$

and $\frac{1}{n-1}\tilde{X}^t\tilde{X} is the so called variance-covariance matrix.

### Example
```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
n = nrow(x)
I = diag(rep(1,n))
H = matrix(1,n,n)/n
round(t(x) %*% (I-H) %*% x / (n-1),6)
```
We get the same as if we do
```{r}
var(x)
```
## Regression to the Origin

Having only one predictor, it means, $X_{n \times 1}$ the value of $\beta$ that minimize  $\lVert y_{n \times 1} - X_{n \times 1}\beta_{p \times 1} \rVert$ is $$ \hat{\beta}=\frac{<y,x>}{<x,x>}$$
It takes the line that going through the origin best fit the data. But in order to do that we have to have the data centered. So to do that we need to transform $x$ and $y$: $$ \tilde{y} =  (I - J_n(J_n^tJ_n)^{-1}J_n^t)y$$
$$ \tilde{x} =  (I - J_n(J_n^tJ_n)^{-1}J_n^t)x$$
So the value that minimize $\lVert \tilde{y}_{n \times 1} - \tilde{X}_{n \times 1}\gamma_{p \times 1} \rVert$ is $$\hat{\gamma}=\frac{\hat{\rho}_{xy}\hat{\sigma}_y}{\hat{\sigma}_x} $$ with $\hat{\rho}_{xy}$ the empirical correlation.

```{r}
library(UsingR)
data(diamond)
y = diamond$price
x = diamond$carat
yc = y - mean(y)
xc = x - mean(x)
sum(yc*xc)/sum(xc*xc)
```
Same value that if we do 
```{r}
lm(yc~xc -1)
```
And same value if we do
```{r}
cor(y,x)*sd(y)/sd(x)
```
## Connection with linear regression

We want to minimize $$ \lVert y_{n \times 1} - (\beta_0J_n + \beta_1X_{n \times 1}) \rVert $$
And the values that minimizes that equation are $$ \hat{\beta}_1 = \hat{\rho}_{yx}\frac{\hat{\sigma}_y}{\hat{\sigma}_x} \quad \hat{\beta}_0 = \bar{y}-\hat{\beta}_1\bar{x} $$

```{r}
library(UsingR)
data(diamond)
y = diamond$price
x = diamond$carat
yc = y - mean(y)
xc = x - mean(x)
```

We can use lm function

```{r}
lm(y ~ x)
plot(y ~ x)
abline(lm(y~x))
```
We get the same if we do
```{r}
beta1 = cor(y,x)*sd(y)/sd(x)
beta0 = mean(y) - beta1*mean(x)
beta1
beta0
```

## Prediction

$$ \hat{y} = \hat{\beta_0}J_n + \hat{\beta_1}X $$
```{r}
yhat = beta0 + beta1*x
yhat
```
And the sum of the squared residuals
```{r}
sum((y-yhat)^2)
```
With the lm function it would be
```{r}
fit = lm(y~x)
predict(fit)
```

## Residuals
$$ y - \hat{y} = y - \hat{\beta_0}J_n - \hat{\beta_1}X = e $$
```{r}
e = y - yhat
e
resid(fit)
```

A good plot is to plot the predictor over the residuals. Remember to do a residual plot and whenever you are looking at a residual plot, you are trying to find any sort of systematic patterns.

```{r}
plot(x,e)
abline(h=0)
```

## Generalization

```{r}
t = seq(0,1, length = 1000)
y = t + 2*t^2
x=t
lm(y ~ x - 1)
```
## Least squares

We want to minimize $\lVert y_{n \times 1} - X_{n \times p}\beta_{p\times 1} \rVert$ being $p \leq n$ and full column rank. The value of $\beta$ that minimizes this equation is $$ \hat{\beta} = (X^tX)^{-1}X^ty $$

```{r}
data("swiss")
head(swiss)
```
```{r}
x = as.matrix(cbind(1,swiss[,-1]))
y = swiss$Fertility
beta=solve(t(x)%*%x) %*% t(x) %*% y
beta
```
We get the same than with lm. The NA in x1 is because we have include into x a column of 1 to be able to do it manually.
```{r}
lm(y~x)
```

