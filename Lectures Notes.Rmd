---
title: "R Notebook"
output: 
  html_notebook: 
    toc: yes
---

# Week 1
## Matrix Derivatives

If we want to explain a vector $y_{n \times 1}$ with $X_{n \times p}$, we need a vector $\beta_{p \times 1}$ that minimize $$\lVert y_{n \times 1} - X_{n \times p}\beta_{p \times 1} \rVert$$
And the minimum of this expression is $$ \hat{\beta} = (x^tx)^{-1}x^ty $$

### Example
```{r}
data(mtcars)
head(mtcars)
```
We define our y, x and $\beta$ variables

\%\*\% matrix multiplication

solve(a) calculates inverse of a

solve(a,b) solves equation a %*% x = b

```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
beta = solve(t(x) %*% x) %*% t(x) %*% y
beta
```
We can check that we obtain the same vector than if we use the function lm
```{r}
coef(lm(mpg ~ wt + hp, data = mtcars))
```
## Centering by matrix multiplication
Let $J_n$ be a vector full of 1 of length n. Then, the matrix $$ (I - J_n(J_n^tJ_n)^{-1}J_n^t) = (I - \frac{1}{n}J_{n \times n}) $$ with $J_{n \times n}$ a matrix full of 1, multiplying $$ (I - \frac{1}{n}J_{n \times n})X_{n \times p} $$ will mean-center the columns of X

### Example

```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
n = nrow(x)
I = diag(rep(1,n))
H = matrix(1,n,n)/n
xc = (I-H) %*% x
```

If we calculate the mean by columns then we get zeros

```{r}
apply(xc,2,mean)
```
Same thing the other way around

```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
p = 3
I = diag(rep(1,p))
H = matrix(1,p,p)/p
xc = x %*%(I-H) 
apply(xc,1,mean)
```
 More efficient way using function sweep
 
```{r}
xc2 = sweep(x,2,apply(x,2,mean))
apply(xc2,2,mean)
```
## Variance via matrix multiplication
 
$$ S^2 = \frac{\sum(y_i-\bar{y})^2}{n-1} = \frac{\lVert y - \bar{y}J_n \rVert^2}{n-1} = \frac{1}{n-1}\tilde{y}^t\tilde{y} $$ where $\tilde{y}=(I - J_n(J_n^tJ_n)^{-1}J_n^t)y$

Then $$ S^2 = y^t(I - J_n(J_n^tJ_n)y $$

If we do it with $X_{n \times p}$ then $$ X^t(I - J_n(J_n^tJ_n)X = \tilde{X}^t\tilde{X} $$

and $\frac{1}{n-1}\tilde{X}^t\tilde{X} is the so called variance-covariance matrix.

### Example
```{r}
y = mtcars$mpg
x = cbind(1,mtcars$wt, mtcars$hp)
n = nrow(x)
I = diag(rep(1,n))
H = matrix(1,n,n)/n
round(t(x) %*% (I-H) %*% x / (n-1),6)
```
We get the same as if we do
```{r}
var(x)
```
## Regression to the Origin

Having only one predictor, it means, $X_{n \times 1}$ the value of $\beta$ that minimize  $\lVert y_{n \times 1} - X_{n \times 1}\beta_{p \times 1} \rVert$ is $$ \hat{\beta}=\frac{<y,x>}{<x,x>}$$
It takes the line that going through the origin best fit the data. But in order to do that we have to have the data centered. So to do that we need to transform $x$ and $y$: $$ \tilde{y} =  (I - J_n(J_n^tJ_n)^{-1}J_n^t)y$$
$$ \tilde{x} =  (I - J_n(J_n^tJ_n)^{-1}J_n^t)x$$
So the value that minimize $\lVert \tilde{y}_{n \times 1} - \tilde{X}_{n \times 1}\gamma_{p \times 1} \rVert$ is $$\hat{\gamma}=\frac{\hat{\rho}_{xy}\hat{\sigma}_y}{\hat{\sigma}_x} $$ with $\hat{\rho}_{xy}$ the empirical correlation.

```{r}
library(UsingR)
data(diamond)
y = diamond$price
x = diamond$carat
yc = y - mean(y)
xc = x - mean(x)
sum(yc*xc)/sum(xc*xc)
```
Same value that if we do 
```{r}
lm(yc~xc -1)
```
And same value if we do
```{r}
cor(y,x)*sd(y)/sd(x)
```

